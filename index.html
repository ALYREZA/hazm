<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Hazm by nournia</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Hazm</h1>
          <h2>Python library for digesting Persian text.</h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/nournia/hazm/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/nournia/hazm/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/nournia/hazm" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
          <h1>
<a name="hazm" class="anchor" href="#hazm"><span class="octicon octicon-link"></span></a>Hazm</h1>

<p>Python library for digesting Persian text.</p>

<ul>
<li>Python 3.3 and 2.7 compatible</li>
<li>NLTK inspired</li>
</ul><h2>
<a name="installation" class="anchor" href="#installation"><span class="octicon octicon-link"></span></a>Installation</h2>

<pre><code>pip install hazm
</code></pre>

<h2>
<a name="usage" class="anchor" href="#usage"><span class="octicon octicon-link"></span></a>Usage</h2>

<pre><code>&gt;&gt;&gt; from hazm import sent_tokenize, word_tokenize
&gt;&gt;&gt; sent_tokenize('جدا کردن ساده است. تقریبا البته!')
['جدا کردن ساده است.', 'تقریبا البته!']
&gt;&gt;&gt; word_tokenize('این جمله معمولی است.')
['این', 'جمله', 'معمولی', 'است', '.']

&gt;&gt;&gt; from hazm import Stemmer, Lemmatizer
&gt;&gt;&gt; stemmer = Stemmer()
&gt;&gt;&gt; stemmer.stem('کتاب‌ها')
'کتاب'
&gt;&gt;&gt; lemmatizer = Lemmatizer()
&gt;&gt;&gt; lemmatizer.lemmatize('می‌روم')
'رفت#رو'
</code></pre>

<h2>
<a name="tests" class="anchor" href="#tests"><span class="octicon octicon-link"></span></a>Tests</h2>

<pre><code>python3 -m doctest README.md
</code></pre>
        </section>

        <footer>
          Hazm is maintained by <a href="https://github.com/nournia">nournia</a><br>
          This page was generated by <a href="http://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>